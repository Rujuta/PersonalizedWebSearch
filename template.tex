\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{url}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}

\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\parskip 12pt 
\setlength{\parindent}{0in}

\pagestyle{myheadings} 

\title{Personalized Web Search}

\author{Biman Gujral (bgujral1, bgujral1@jhu.edu), Rujuta Deshpande (rdeshpa3,rdeshpa3@jhu.edu)}
\date{10/31/2014}

\begin{document}
\maketitle

\section{Abstract}
This project aims to improve web search results through personalization. It involves re-ranking of results on the basis of user profiles comprising their past searches and actions. \newline
The features used are based upon the short term and long term user search history. Short term history is the user's search queries and actions(dwell time for URL and clicks on URL) in the same search session. Long term refers to a user's search behavior observed over multiple sessions over a long term period.\newline 
This problem lies in the Learning to Rank class of problems. 

\section{Methods}
\subsection{Learning and Prediction} The problem of re-ranking web search results to obtain personalized results falls under Learning to Rank which involves use of machine learning for ranking in information retrieval. This approach is achieved usually through Pointwise, Pairwise or Listwise approach. \texttt{<ADD>}
We will start by implementing LambdaMART which is a listwise Learning to Rank algorithm as it has been ranked
\subsection{Evaluation} The task involves re-ranking of search results in accordance with user's history. Therefore, the evaluation metric used is NDCG (Normalized Discounted Cumulative Gain). It is a metric between 0 and 1 that evaluates the ranking order. It is given by:
\begin{flalign*}
NDCG_k = \frac{DCG_k}{IDCG_k}
\end{flalign*}
where k denotes documents uptil rank k and DCG is Discounted Cumulative Gain, given by:
\begin{flalign*}
DCG_k = \sum_{i=1}^k{\frac{2^{rel_i} - 1}{log_2(i + 1)}}
\end{flalign*}
where rel is the actual relevance of the document provided by labels and i is the rank given by the algorithm. Thus, a highly relevant document ranked later in the list will result in penalization and a low DCG. The Ideal DCG or IDCG gives the best ranking in accordance with the relevance values. Therefore, NDCG = 1 when the ideal ranking is obtained. For our experiment, NDCG is calculated on the newly ranked list of URLs created by the algorithm. Thus, the learned ranking are evaluated with respect to the relevance labels obtained on the basis of clicks and dwell time on a document of a user for a particular query.

\section{Resources}
% What resources will you use and how will you get them? - Libraries, Data etc 
Our project is essentially a kaggle competition. Hence, we have obtained the data from kaggle itself. It consists of the web search logs of Yandex, a Russian search engine, collected over a period of 30 days. The first 27 days form the train data and the remaining 3 days form the test data. \newline
The training data is a stream of logs. Each log line includes a SESSION ID,a type of record  - which essentially indicates the type of log (query or click or session meta data). Based on the type of record, there is a USER ID, QUERY ID, URL ID. The field TIMEPASSED in the record, indicates the time that has passed uptill that record, from the start of that session. 

\subsection{Tools}
We plan to use Python for our implementation and scikit-learn for any machine learning libraries we might need. \newline
We might use a database to store our learned parameters - SQLite/MongoDB
\section{Milestones}
\subsection{Must achieve}
Our aim is to re-rank the returned URLs to obtain a high NDCG score.  
\subsection{Expected to achieve}

\subsection{Would like to achieve}
Comparison of various machine learning algorithms used for re-ranking. A study of whether long term history alone can be useful for making predictions. 

\section{Final Writeup}
The final write up will consist of the following:\newline
\begin{itemize}
\item Abstract
\item Related work
\item Our approach 
\item Methodology
\item Evaluation Technique
\item Graphs
\item References
\end{itemize}

\section{Bibliography}
\begin{itemize}
\item Milad Shokouhi, Ryen W. White, Paul Bennett, and Filip Radlinski. Fighting search engine amnesia: reranking repeated results. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval (SIGIR '13).
\item David Sontag, Kevyn Collins-Thompson, Paul N. Bennett, Ryen W. White, Susan Dumais, and Bodo Billerbeck. Probabilistic models for personalizing web search. In Proceedings of the fifth ACM international conference on Web search and data mining (WSDM '12).
\item Zhicheng Dou, Ruihua Song, and Ji-Rong Wen.  A large-scale evaluation and analysis of personalized search strategies. In Proceedings of the 16th international conference on World Wide Web (WWW '07).
\item Feng Qiu and Junghoo Cho. Automatic identification of user interest for personalized search. In Proceedings of the 15th international conference on World Wide Web (WWW '06).
\end{itemize}

\end{document}

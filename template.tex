\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{amsmath}

\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\parskip 12pt 
\setlength{\parindent}{0in}

\pagestyle{myheadings} 

\title{Personalized Web Search}

\author{Biman Gujral (bgujral1, bgujral1@jhu.edu), Rujuta Deshpande (rdeshpa3,rdeshpa3@jhu.edu)}
\date{10/31/2014}

\begin{document}
\maketitle

\section{Abstract}
% Clearly explain your idea. What are we exactly trying to do. What has been done? Issues that exist 
We are trying to improve web search results, by re-ranking pages that are returned by a web search by personalizing them according to a user's preferences. %% Write about different methods - long term/ short term session based/ mix of the two. 


\section{Methods}
% Explain the methods you will be using and why they are appropriate.- ML methods - algorithm? Objective function? Evaluation techniques? 
\subsection{Evaluation} The task involves re-ranking of search results in accordance with user's history. Therefore, the evaluation metric used is NDCG (Normalized Discounted Cumulative Gain). It is a metric between 0 and 1 that evaluates the ranking order. It is given by:
\begin{flalign*}
NDCG_k = \frac{DCG_k}{IDCG_k}
\end{flalign*}
where k denotes documents uptil rank k and DCG is Discounted Cumulative Gain, given by:
\begin{flalign*}
DCG_k = rel_1 + \sum_{i=2}^p{\frac{rel_i}{log_2(i)}}
\end{flalign*}
where rel is the actual relevance of the document provided by labels and i is the rank given by the algorithm. Thus, a highly relevant document ranked later in the list will result in penalization and a low DCG. The Ideal DCG or IDCG gives the best ranking in accordance with the relevance values. Therefore, NDCG = 1 when the ideal ranking is obtained. For our experiment, NDCG is calculated on the newly ranked list of URLs created by the algorithm. Thus, the learned ranking are evaluated with respect to the relevance labels obtained on the basis of clicks and dwell time on a document of a user for a particular query.

\section{Resources}
% What resources will you use and how will you get them? - Libraries, Data etc 
Our project is essentially a kaggle competition. Hence, we have obtained the data from kaggle itself. It consists of the web search logs of Yandex, a Russian search engine, collected over a period of two months. \newline
%%Explain Training Data Format 
\section{Milestones}
%%Explain Test Data format and what we will be giving out as output
%% How do we plan to Evaluate our results
\subsection{Must achieve}
%% Some kind of metric ? Probably accuracy, improvement in current techniques, ability to classify something correctly. 
%%%%Explain Test Data format and what we will be giving out as output
%% How do we plan to Evaluate our result
\subsection{Expected to achieve}
% Improvement?? Comparitive Study ? 
\subsection{Would like to achieve}
% Enhancement over previously done work.??

\section{Final Writeup}
% What will appear in the final writeup.

\section{Bibliography}
% A list of the papers relevant to this project.

\end{document}

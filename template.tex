\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{url}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}

\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\parskip 1pt 
\setlength{\parindent}{0in}

\pagestyle{myheadings} 

\title{Personalized Web Search}

\author{Biman Gujral (bgujral1, bgujral1@jhu.edu), Rujuta Deshpande (rdeshpa3,rdeshpa3@jhu.edu)}
\date{10/31/2014}

\begin{document}
\maketitle

\section{Abstract}
We are trying to improve web search results, by re-ranking pages that are returned by a web search by personalizing them according to a user's preferences. \newline
We plan to include both short term and long term user search history in order to generate the new rankings. The short term history is the user's search behavior (dwell time and click through data) in the same search session, whereas long term refers to a user's search behavior observed over multiple sessions over a long term period.\newline 
Currently, there is literature demonstrating re-ranking based only on long term user history as well as short term and long term history. There are papers explaining that personalized search would be ideal solely in case of ambiguity of search queries. We are trying to identify this factor when we re-rank our documents. 
 


\section{Methods}
% Explain the methods you will be using and why they are appropriate.- ML methods - algorithm? Objective function? Evaluation techniques? 
\subsection{Evaluation} The task involves re-ranking of search results in accordance with user's history. Therefore, the evaluation metric used is NDCG (Normalized Discounted Cumulative Gain). It is a metric between 0 and 1 that evaluates the ranking order. It is given by:
\begin{flalign*}
NDCG_k = \frac{DCG_k}{IDCG_k}
\end{flalign*}
where k denotes documents uptil rank k and DCG is Discounted Cumulative Gain, given by:
\begin{flalign*}
DCG_k = rel_1 + \sum_{i=2}^p{\frac{rel_i}{log_2(i)}}
\end{flalign*}
where rel is the actual relevance of the document provided by labels and i is the rank given by the algorithm. Thus, a highly relevant document ranked later in the list will result in penalization and a low DCG. The Ideal DCG or IDCG gives the best ranking in accordance with the relevance values. Therefore, NDCG = 1 when the ideal ranking is obtained. For our experiment, NDCG is calculated on the newly ranked list of URLs created by the algorithm. Thus, the learned ranking are evaluated with respect to the relevance labels obtained on the basis of clicks and dwell time on a document of a user for a particular query.

\section{Resources}
% What resources will you use and how will you get them? - Libraries, Data etc 
Our project is essentially a kaggle competition. Hence, we have obtained the data from kaggle itself. It consists of the web search logs of Yandex, a Russian search engine, collected over a period of 30 days. The first 27 days form the train data and the remaining 3 days form the test data. \newline
The training data is a stream of logs. Each log line includes a SESSION ID,a type of record  - which essentially indicates the type of log (query or click or session meta data). Based on the type of record, there is a USER ID, QUERY ID, URL ID. The field TIMEPASSED in the record, indicates the time that has passed uptill that record, from the start of that session. 

\subsection{Tools}
We plan to use Python for our implementation and scikit-learn for any machine learning libraries we might need. \newline
We might use a database to store our learned parameters - SQLite/MongoDB
\section{Milestones}
\subsection{Must achieve}
Evaluation is NDCG and we aim to achieve an ordering of the URLs sorted according to decreasing relevance for a particular user. This would be reflected by an NDCG score close to 1.     
\subsection{Expected to achieve}
We intend to do a comparitive study of the algorithm we are using - LambdaMART with at least two other 'Learning to Rank' algorithms that we think would suit our problem. 
\subsection{Would like to achieve}
The ideal expected NDCG score is around 0.8065. We would like to re-engineer some features and possibly modify the algorithm to achieve a higher score.  
\section{Final Writeup}
\subsection*{•}
\vspace{-5mm}
\begin{itemize}\setlength{\itemsep}{-5pt}
\item Abstract and General Approach
\item Related work and our approach in comparison to them
\item Explanation of Feature Engineering, Algorithm and Evaluation metrics  
\end{itemize}
\noindent
\section{Bibliography}
\subsection*{•}
\vspace{-5mm}
\begin{enumerate}\setlength{\itemsep}{-5pt}
\item David Sontag, Kevyn Collins-Thompson, Paul N. Bennett, Ryen W. White, Susan Dumais, and Bodo Billerbeck. Probabilistic models for personalizing web search. In Proceedings of the fifth ACM international conference on Web search and data mining (WSDM '12).
\item Zhicheng Dou, Ruihua Song, and Ji-Rong Wen.  A large-scale evaluation and analysis of personalized search strategies. In Proceedings of the 16th international conference on World Wide Web (WWW '07).
\item Learning to Rank \texttt{http://en.wikipedia.org/wiki/Learning\_to\_rank}
\item Dataiku's winning Solution to the problem \texttt{http://research.microsoft.com/en-us/um/people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf}

\end{enumerate}

\end{document}

\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref} 
\usepackage{color}
\usepackage{url}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}

\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 165mm

\parskip 1pt 
\setlength{\parindent}{0in}

\pagestyle{myheadings} 

\title{\vspace{-18mm}CS600.475: Proposal\\
\Large{Machine Learning Systems: Personalized Web Search}}
\author{\normalsize{Biman Gujral (bgujral1, bgujral1@jhu.edu) \hspace{5 mm}  Rujuta Deshpande (rdeshpa3,rdeshpa3@jhu.edu)}\\
\small{October 31, 2014}}
\date{}

\begin{document}
\maketitle

\section{Abstract}
This project aims to improve web search results through personalization. It involves re-ranking of results on the basis of user profiles comprising their past searches and actions. \newline
The features used are based upon the short term and long term user search history. Short term history is the user's search queries and actions(dwell time for URL and clicks on URL) in the same search session. Long term refers to a user's search behavior observed over multiple sessions over a long term period.This problem lies in the Learning to Rank class of problems.

\section{Methods}
\subsection{Learning and Prediction} The problem of re-ranking web search results to obtain personalized results falls under Learning to Rank which involves algorithms to generate rankings for information retrieval based on rankings in training data. These algorithms are usually implemented using either Pointwise, Pairwise or Listwise approach. We will start by implementing LambdaMART, a listwise approach, as it has been observed to perform the best in the paper that we are initially following. 
 
\subsection{Evaluation} The evaluation metric used is NDCG (Normalized Discounted Cumulative Gain). It is a metric between 0 and 1 that evaluates the ranking order. It is given by:
\begin{flalign*}
NDCG_k = \frac{DCG_k}{IDCG_k}
\end{flalign*}
where k denotes documents uptil rank k and DCG is Discounted Cumulative Gain, given by:
\begin{flalign*}
DCG_k = \sum_{i=1}^k{\frac{2^{rel_i} - 1}{log_2(i + 1)}}
\end{flalign*}
where rel is the actual relevance of the document provided by labels and i is the rank given by the algorithm. Thus, a highly relevant document ranked later in the list will result in penalization and a low DCG. The Ideal DCG or IDCG gives the best ranking in accordance with the relevance values. Therefore, NDCG = 1 when the ideal ranking is obtained. 

\section{Resources}
\subsection{Data}
Our project idea is obtained from a Kaggle competition. Hence, data is taken from Kaggle. It consists of web search logs of Yandex, a Russian search engine, collected over a period of 30 days. The first 27 days form the train data and the remaining 3 days form the test data. \newline
The training data is a stream of logs. Each log line includes a SESSION ID, a type of record  - which indicates if it is information about a query(Q), a click(C) or meta data(M). Based on the type of record, there are other fields like \textsl{UserID, QueryID and terms, URLID-DomainID and SERPID}. The field \textsl{TimePassed}, indicates the time that has passed since sessions start until the click was made which helps compute the dwell time.

\subsection{Tools}
We plan to use Python for algorithm implementation and scikit-learn for any machine learning libraries we may need. We may also use a database to work with the large datasets and store our learned parameters - SQLite.

\section{Milestones}
\subsection{Must achieve}
Since NDCG is used as evaluation metrics, we aim to achieve ordering of the URLs sorted according to decreasing relevance for a particular user. This would be reflected by an NDCG score close to 1. We will attempt to achieve this using the LambdaMART algorithm. We will use the ranklib library's Java implementation of LambdaMART for our task. 
\subsection{Expected to achieve}
The Kaggle competition winners have experimented with various features - both user specific and global. Some of them are the non-personalized rank of the URL, the frequency of query, the position of the query within the user's session, whether the URL has been missed or skipped, the relevance of the snippet displayed in search results and the like. In total, they have used about 90 features. Many features are obtained as a conditional probability of getting a URL conditioned on some predicate. This predicate is a function of the a number of global and user specific variables. \newline
Our aim is to experiment with a subset of these features and compare the accuracy we achieve. We would be able to determine which features influence ranking greatly and whicj don't. We also plan to use not just LambdaMART but a few other Learning to Rank algorithms - RankNet, AdaRank, Random Forests and compare their results to the ones returned by LambdaMART.
\subsection{Would like to achieve}
One of the papers we read, spoke about whether personalization is necessary for every query. We would like to improve upon the search results, by only personalizing when needed. Thus, at all times, we would get optimum results. 
\section{Final Writeup} The final writeup will disuss in detail the algorithms and experimental setup and the results will be presented. Some broad headers will be:\\
\vspace{-5mm} 
\begin{itemize}\setlength{\itemsep}{-5pt}
\item Abstract and General Approach
\item Related work and our approach in comparison to them
\item Explanation of Feature Engineering, Algorithm, Evaluation metrics and Result  
\end{itemize}
\noindent
\section{Bibliography}
\subsection*{â€¢}
\vspace{-5mm}
\begin{enumerate}\setlength{\itemsep}{-5pt}
\item David Sontag, Kevyn Collins-Thompson, Paul N. Bennett, Ryen W. White, Susan Dumais, and Bodo Billerbeck. Probabilistic models for personalizing web search. In Proceedings of the fifth ACM international conference on Web search and data mining (WSDM '12).
\item Zhicheng Dou, Ruihua Song, and Ji-Rong Wen.  A large-scale evaluation and analysis of personalized search strategies. In Proceedings of the 16th international conference on World Wide Web (WWW '07).
\item Learning to Rank \texttt{http://en.wikipedia.org/wiki/Learning\_to\_rank}
\item Dataiku's winning Solution to the problem \texttt{http://research.microsoft.com/en-us/um/ \\ people/nickcr/wscd2014/papers/wscdchallenge2014dataiku.pdf}

\end{enumerate}

\end{document}
